# Model configurations for each case study
#
# DESIGN PHILOSOPHY: Each experiment is designed so the original prompt works well
# on the old model but degrades measurably on the new model, triggering the CODA
# pipeline's Classify -> Optimize -> Validate phases.
#
# Case A: Within-provider upgrade. GPT-4 -> GPT-4o. Expected PPI: 85-94 (orange).
#   GPT-4o is more conversational/verbose, triggering instruction drift.
#
# Case B: Within-provider capability shift. Claude Sonnet 4 -> Claude Haiku 4.5.
#   Haiku is faster but less capable at complex tool orchestration, triggering
#   tool-calling failures and source parameter omission. Expected PPI: 70-84 (red).
#
# Case C: Cross-provider migration. GPT-4 -> Claude Haiku 4.5.
#   Maximum divergence: different provider AND smaller model. GPT-4-specific CoT
#   scaffolding breaks on Claude. Expected PPI: <70 (critical).
#
# Case D: Cross-provider capability downgrade. Claude Sonnet 4 -> GPT-4o-mini.
#   Multi-step arithmetic cost estimation requires reasoning depth that GPT-4o-mini
#   lacks. Optimization cannot fix fundamental arithmetic capability gaps.
#   Expected PPI: <70 (critical, unfixable).
#
# Case E: Within-provider downgrade. GPT-4 -> GPT-4o-mini.
#   Incident report formatting task. Format compliance degrades (missing sections,
#   word limit violations, metadata omissions) but is recoverable via prompt
#   optimization. Expected PPI: 70-84 (red, optimizable to green).

cases:
  a:
    name: "System Prompt Migration"
    description: "Customer service agent migrated from GPT-4 to GPT-4o"
    old_model:
      provider: openai
      model: gpt-4
      temperature: 0.3
      max_tokens: 300
    new_model:
      provider: openai
      model: gpt-4o
      temperature: 0.3
      max_tokens: 300
    prompt_file: prompts/case_a_original.txt
    test_suite: test_suites/case_a_tests.jsonl
    evaluator_config:
      format_checks:
        - type: max_word_count
          value: 150
        - type: ends_with
          value: "Is there anything else I can help with?"
        - type: must_not_contain
          values: ["- ", "* ", "1. ", "2. ", "3. "]
          label: "no_bullet_points"
      instruction_checks:
        - type: must_not_contain_topics
          topics: ["CompetitorX", "CompetitorY", "CompetitorZ"]
          label: "competitor_mention"
        - type: must_contain
          value: "Let me connect you with a specialist"
          condition: "when ground_truth contains 'ESCALATE'"
          label: "escalation_compliance"
      metric_weights:
        task_accuracy: 0.30
        format_compliance: 0.20
        instruction_adherence: 0.25
        consistency: 0.15
        latency: 0.05
        cost: 0.05

  b:
    name: "Tool-Calling Capability Shift"
    description: "Search/DB agent migrated from Claude Sonnet 4 to Claude Haiku 4.5"
    old_model:
      provider: anthropic
      model: claude-sonnet-4-20250514
      temperature: 0.2
      max_tokens: 1024
    new_model:
      provider: anthropic
      model: claude-haiku-4-5-20251001
      temperature: 0.2
      max_tokens: 1024
    prompt_file: prompts/case_b_original.json
    test_suite: test_suites/case_b_tests.jsonl
    evaluator_config:
      tool_calling:
        validate_schema: true
        required_params_strict: true
      metric_weights:
        task_accuracy: 0.30
        tool_calling_success: 0.30
        format_compliance: 0.15
        instruction_adherence: 0.10
        consistency: 0.10
        latency: 0.05

  c:
    name: "Cross-Provider CoT Migration"
    description: "Financial analysis agent migrated from GPT-4 to Claude Haiku 4.5"
    old_model:
      provider: openai
      model: gpt-4
      temperature: 0.1
      max_tokens: 2048
    new_model:
      provider: anthropic
      model: claude-haiku-4-5-20251001
      temperature: 0.1
      max_tokens: 2048
    prompt_file: prompts/case_c_original.txt
    test_suite: test_suites/case_c_tests.jsonl
    evaluator_config:
      reasoning_checks:
        check_intermediate_steps: true
        require_final_answer_format: "json"
      metric_weights:
        task_accuracy: 0.35
        reasoning_quality: 0.25
        format_compliance: 0.15
        consistency: 0.10
        instruction_adherence: 0.10
        latency: 0.05

  d:
    name: "Cross-Provider Format Downgrade"
    description: "Support response writer migrated from Claude Sonnet 4 to GPT-4o-mini"
    old_model:
      provider: anthropic
      model: claude-sonnet-4-20250514
      temperature: 0.2
      max_tokens: 1024
    new_model:
      provider: openai
      model: gpt-4o-mini
      temperature: 0.2
      max_tokens: 1024
    prompt_file: prompts/case_d_original.txt
    test_suite: test_suites/case_d_tests.jsonl
    evaluator_config:
      format_checks:
        - type: must_have_header
          value: "## SUPPORT RESPONSE"
        - type: must_have_sections
          values: ["ISSUE ACKNOWLEDGMENT", "ROOT CAUSE ANALYSIS", "RESOLUTION STEPS", "FOLLOW-UP ACTIONS"]
        - type: must_end_with
          value: "--- Response by Support Team ---"
        - type: must_use_numbered_steps
      metric_weights:
        format_compliance: 0.30
        instruction_adherence: 0.10
        task_accuracy: 0.50
        consistency: 0.05
        latency: 0.05

  e:
    name: "Within-Provider Format Downgrade"
    description: "Incident report writer migrated from GPT-4 to GPT-4o-mini"
    old_model:
      provider: openai
      model: gpt-4
      temperature: 0.2
      max_tokens: 1024
    new_model:
      provider: openai
      model: gpt-4o-mini
      temperature: 0.2
      max_tokens: 1024
    prompt_file: prompts/case_e_original.txt
    test_suite: test_suites/case_e_tests.jsonl
    evaluator_config:
      format_checks:
        - type: must_have_header
          value: "INCIDENT REPORT"
        - type: must_have_sections
          values: ["SUMMARY", "IMPACT", "ROOT CAUSE", "RESOLUTION", "FOLLOW-UP"]
        - type: must_end_with
          value: "--- End of Report ---"
      metric_weights:
        format_compliance: 0.20
        instruction_adherence: 0.10
        task_accuracy: 0.45
        consistency: 0.10
        latency: 0.05
        cost: 0.10

# Default settings
defaults:
  consistency_runs: 5          # Number of runs per test case for consistency measurement
  degradation_threshold: 0.05  # 5% relative decline triggers flagging
  optimization_max_iterations: 10
  optimization_patience: 3     # Stop if no improvement for N iterations
