{"id": "e001", "input": "At 3:47 AM UTC on January 15, 2026, our primary PostgreSQL database cluster became completely unresponsive. All customer-facing services returned 500 errors. The incident was reported by on-call engineer Maria Chen after automated monitoring triggered PagerDuty alerts. The root cause was traced to a runaway vacuum process that consumed all available disk IOPS, causing connection timeouts across the cluster. Resolution involved killing the vacuum process, scaling up IOPS allocation, and performing a rolling restart of the connection pool. Total downtime was 47 minutes. No data was lost. Approximately 12,000 customers were unable to access the platform during the outage.", "expected": {"severity": "P1", "status": "RESOLVED", "reporter": "Maria Chen", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p1", "resolved", "database"]}
{"id": "e002", "input": "On February 3, 2026, starting at 2:15 PM UTC, our primary payment processor Stripe began returning elevated error rates with approximately 15% of transactions failing. The issue was detected by engineer Jake Torres through the transaction monitoring dashboard. We activated our backup payment gateway at 2:32 PM UTC, which reduced customer impact to under 2%. The root cause was a Stripe infrastructure issue in their EU-West region. Stripe resolved the issue at 4:10 PM UTC. During the incident, approximately 2,300 transactions failed before the fallback was activated. Revenue impact estimated at $45,000 in delayed transactions. Duration from detection to full resolution was 1 hour 55 minutes.", "expected": {"severity": "P2", "status": "MITIGATED", "reporter": "Jake Torres", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p2", "mitigated", "payments"]}
{"id": "e003", "input": "March 10, 2026 -- QA lead Sandra Kim noticed that the search autocomplete feature on the website was returning stale results (24-48 hours old) instead of real-time suggestions. The issue affected the autocomplete dropdown only; full search results were accurate. Root cause: a cron job responsible for refreshing the Elasticsearch autocomplete index had silently failed 2 days prior due to a disk space issue on the indexing server. Fix: cleared disk space, restarted the cron job, added disk space monitoring alert. The autocomplete index was fully refreshed within 30 minutes. Estimated that fewer than 500 users per day were affected. Duration of stale data: approximately 48 hours. Duration of fix: 30 minutes.", "expected": {"severity": "P3", "status": "RESOLVED", "reporter": "Sandra Kim", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p3", "resolved", "search"]}
{"id": "e004", "input": "Frontend developer Leo Park reported on April 5, 2026 that the settings page had a minor CSS alignment issue where the 'Save' button overlapped with the 'Cancel' button on screen widths between 768px and 800px. No functionality was affected -- both buttons remained clickable. The issue was introduced in deploy v2.34.1 when a flex-wrap rule was removed during CSS cleanup. Fix was a one-line CSS change adding the flex-wrap rule back. Deployed in hotfix v2.34.2 within 2 hours of report. Estimated fewer than 50 users per day use settings on affected screen widths.", "expected": {"severity": "P4", "status": "RESOLVED", "reporter": "Leo Park", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p4", "resolved", "css"]}
{"id": "e005", "input": "CRITICAL -- January 28, 2026, 11:02 PM UTC. The authentication service (Auth0-based) went completely down. No users could log in, sign up, or access any authenticated endpoints. All API calls requiring authentication returned 401 errors. Reported by SRE team lead Raj Patel when the monitoring dashboard went red across all services. We enabled read-only mode for previously authenticated sessions at 11:15 PM. Root cause: Auth0 experienced a regional outage in US-East-1. Auth0 restored partial service at 12:30 AM and full service at 1:47 AM on January 29. Total authentication outage: 2 hours 45 minutes. Read-only mode active: 2 hours 32 minutes. All 45,000 active users were affected.", "expected": {"severity": "P1", "status": "MITIGATED", "reporter": "Raj Patel", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p1", "mitigated", "auth"]}
{"id": "e006", "input": "February 15, 2026 -- Backend engineer Nina Kowalski discovered that our REST API rate limiting was misconfigured after a deployment, causing approximately 30% of legitimate API users to receive 429 Too Many Requests errors. The rate limit threshold had been accidentally lowered from 1000 req/min to 100 req/min in a config file change. The issue started at 9:00 AM UTC with the deployment and was identified at 10:15 AM UTC. Fix: reverted the config change and redeployed. Full resolution at 10:45 AM UTC. Approximately 800 API users were affected over the 1 hour 45 minute window.", "expected": {"severity": "P2", "status": "RESOLVED", "reporter": "Nina Kowalski", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p2", "resolved", "api"]}
{"id": "e007", "input": "March 22, 2026 -- DevOps engineer Carlos Rivera flagged that email notifications (order confirmations, password resets, welcome emails) were experiencing intermittent delays of 15-45 minutes instead of the normal sub-minute delivery. The issue has been ongoing since approximately March 20 and is still active. Root cause: our SendGrid account hit a sending reputation threshold causing automatic throttling. Approximately 8% of emails are delayed; critical transactional emails (password resets) are prioritized and unaffected. We have opened a ticket with SendGrid and are awaiting their response. No workaround has been implemented yet.", "expected": {"severity": "P3", "status": "ONGOING", "reporter": "Carlos Rivera", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p3", "ongoing", "email"]}
{"id": "e008", "input": "April 1, 2026 -- Product manager Diana Reyes noticed that the admin dashboard loads approximately 20% slower than usual (page load time increased from 2.1s to 2.5s average). The performance degradation appears to have started around March 28. Only the admin dashboard is affected; customer-facing pages show normal performance. The admin dashboard is used by approximately 25 internal staff members. Investigation is ongoing. Initial review by engineer Tom Walsh suggests it may be related to a new analytics widget added in the last sprint, but this has not been confirmed.", "expected": {"severity": "P4", "status": "INVESTIGATING", "reporter": "Diana Reyes", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p4", "investigating", "performance"]}
{"id": "e009", "input": "CRITICAL INCIDENT -- February 8, 2026, 4:12 AM UTC. A network partition in our primary data center caused a split-brain scenario in our Redis cluster, leading to inconsistent session data across application servers. Users experienced random logouts, seeing other users' shopping carts, and duplicate order submissions. Reported by automated anomaly detection, escalated by on-call SRE Yuki Tanaka. Immediate response: disabled the affected Redis nodes and forced all sessions to re-authenticate at 4:25 AM. Full cluster recovery completed at 5:48 AM after manual reconciliation. 3 duplicate orders were identified and reversed. Approximately 8,000 users were affected. Total incident duration: 1 hour 36 minutes.", "expected": {"severity": "P1", "status": "RESOLVED", "reporter": "Yuki Tanaka", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p1", "resolved", "network"]}
{"id": "e010", "input": "March 5, 2026 -- Engineer Priya Sharma reported that our CDN (Cloudflare) cache invalidation was failing silently for image assets. Users were seeing outdated product images after merchants updated them. The issue affected approximately 15% of recently updated product listings. Root cause: a Cloudflare API token had expired, causing cache purge requests to fail with 403 errors that were not properly logged. Fix: renewed the API token and triggered a full cache purge. Added token expiry monitoring. All stale images were refreshed within 20 minutes. The stale cache issue had been present for approximately 5 days before detection. Resolution time from detection: 45 minutes.", "expected": {"severity": "P2", "status": "RESOLVED", "reporter": "Priya Sharma", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p2", "resolved", "cdn"]}
{"id": "e011", "input": "March 18, 2026 -- Mobile developer Alex Nguyen reported that the iOS app (version 4.2.1) crashes when users attempt to upload images larger than 10MB from the photo library. The crash occurs in the image compression module. Android app is unaffected. Approximately 3% of iOS photo uploads are over 10MB. The bug was introduced in version 4.2.0 when the image processing library was updated. Fix: added a pre-upload size check with automatic compression for images over 8MB. Fix deployed in version 4.2.2 via expedited App Store review, available to users within 48 hours of report.", "expected": {"severity": "P3", "status": "RESOLVED", "reporter": "Alex Nguyen", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p3", "resolved", "mobile"]}
{"id": "e012", "input": "February 20, 2026 at 3:30 PM UTC -- Support lead Michael Brown escalated reports from multiple customers that checkout was timing out intermittently. Investigation revealed that our third-party shipping rate calculator API (ShipEngine) was responding with 8-12 second latency instead of the normal 200ms, causing our 10-second checkout timeout to trigger. Approximately 20% of checkout attempts failed. We deployed an emergency patch at 4:15 PM that cached recent shipping rate calculations and fell back to flat-rate shipping when ShipEngine timed out. ShipEngine acknowledged the issue at 5:00 PM and restored normal performance at 6:30 PM. Duration: 3 hours.", "expected": {"severity": "P2", "status": "MITIGATED", "reporter": "Michael Brown", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p2", "mitigated", "checkout"]}
{"id": "e013", "input": "SECURITY INCIDENT -- January 20, 2026, 9:15 AM UTC. Security engineer Liam O'Brien discovered during a routine audit that a misconfigured S3 bucket containing customer export files was publicly accessible. The bucket had been misconfigured since January 12 during an infrastructure migration. The exposed data included customer names, email addresses, and order histories for approximately 2,400 customers. No financial data (credit cards, SSN) was exposed. The bucket was immediately locked down at 9:22 AM. CloudTrail logs showed no unauthorized access to the bucket during the exposure window. All affected customers were notified per our data breach protocol. Total exposure window: 8 days. Time to remediate after detection: 7 minutes.", "expected": {"severity": "P1", "status": "RESOLVED", "reporter": "Liam O'Brien", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p1", "resolved", "security"]}
{"id": "e014", "input": "March 28, 2026 -- Backend engineer Rosa Martinez reported that the notification service was sending duplicate push notifications and emails for order status updates. Approximately 10-15% of notifications were duplicated. Root cause: a race condition in the notification queue consumer when processing concurrent status updates for the same order. The consumer lacked idempotency checks. Fix: added a deduplication key based on order ID and notification type with a 5-minute TTL in Redis. Deployed fix at 2:00 PM UTC. The issue had been occurring intermittently for approximately 2 weeks before being formally reported. No data loss or incorrect notifications -- just duplicates.", "expected": {"severity": "P3", "status": "RESOLVED", "reporter": "Rosa Martinez", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p3", "resolved", "notifications"]}
{"id": "e015", "input": "April 8, 2026 -- Customer support agent Wei Zhang noticed that users in the Asia-Pacific region were seeing timestamps displayed in UTC instead of their local timezone on the order tracking page. All other pages displayed correct local times. The issue was traced to a recent refactor of the order tracking component that removed the timezone conversion call. Approximately 4,000 APAC users per day were affected. Fix: restored the timezone conversion in the order tracking component. Deployed same day. Duration of incorrect display: approximately 5 days before detection. Fix deployment time: 3 hours from report.", "expected": {"severity": "P4", "status": "RESOLVED", "reporter": "Wei Zhang", "section_word_limits": {"SUMMARY": 50, "IMPACT": 75, "ROOT CAUSE": 75, "RESOLUTION": 100, "FOLLOW-UP": 75}, "must_have_report_header": true, "must_have_metadata": true, "must_have_sections": true, "must_end_with_closing": true, "must_not_use_exclamation": true, "must_not_use_speculation": true}, "tags": ["p4", "resolved", "timezone"]}
